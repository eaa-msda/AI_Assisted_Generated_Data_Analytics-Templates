{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IWCgi1alMTJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zTjogvuHMaS"
      },
      "outputs": [],
      "source": [
        "# Errol Ian Ave Acosta\n",
        "# Data Science | Google Colab\n",
        "# Grok 3 Free | Data Science\n",
        "# February 16, 2025\n",
        "\n",
        "\"\"\" Important Notes:\n",
        "\n",
        "Replace 'path/to/your/dataset.csv' with the actual path to your dataset.\n",
        "Adjust numeric_features and categorical_features according to your dataset's features.\n",
        "\n",
        "The SimpleImputer strategies ('mean', 'median', 'constant') should be chosen based on the nature of your data.\n",
        "You might need to tune hyperparameters of the models for better performance,\n",
        "which can be done using techniques like GridSearchCV.\n",
        "\n",
        "This template provides a structured approach to data science projects, but remember,\n",
        "the specifics can vary greatly depending on your dataset and the problem you're solving.\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "# Notes: Ensure the file path is correct, and the dataset is suitable for your analysis.\n",
        "df = pd.read_csv('path/to/your/dataset.csv')\n",
        "\n",
        "# Initial exploration\n",
        "# Notes: Check for basic statistics, data types, missing values, etc.\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Data Cleaning\n",
        "# Notes: Handle missing values, outliers, and incorrect data types.\n",
        "# Example: Impute missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['column_with_missing_values'] = imputer.fit_transform(df[['column_with_missing_values']])\n",
        "\n",
        "# Data Visualization\n",
        "# Notes: Visualize data distribution, correlations, etc.\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap of Features')\n",
        "plt.show()\n",
        "\n",
        "# Distribution of a specific feature\n",
        "sns.histplot(df['feature_name'], kde=True)\n",
        "plt.title('Distribution of Feature Name')\n",
        "plt.show()\n",
        "\n",
        "# Feature Engineering\n",
        "# Notes: Create new features that might be useful for prediction.\n",
        "df['new_feature'] = df['feature1'] * df['feature2']\n",
        "\n",
        "# Pre-processing\n",
        "# Notes: Split data into features and target, then prepare for machine learning.\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing steps for numerical and categorical data\n",
        "numeric_features = ['num_feature1', 'num_feature2']\n",
        "categorical_features = ['cat_feature1', 'cat_feature2']\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Model Pipelines\n",
        "# Notes: Define pipelines for different machine learning models with preprocessing included.\n",
        "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', RandomForestClassifier())])\n",
        "\n",
        "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                              ('classifier', LogisticRegression())])\n",
        "\n",
        "svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', SVC())])\n",
        "\n",
        "# Fit models\n",
        "# Notes: Train models on the training data.\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "# Notes: Use models to predict on test data.\n",
        "y_pred_rf = rf_pipeline.predict(X_test)\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "y_pred_svm = svm_pipeline.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "# Notes: Evaluate models using classification metrics.\n",
        "print(\"Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Logistic Regression:\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "print(\"Support Vector Machine:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "# Notes: Visualize how well the model performs in terms of true positives, false positives, etc.\n",
        "for name, y_pred in [('Random Forest', y_pred_rf), ('Logistic Regression', y_pred_lr), ('SVM', y_pred_svm)]:\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix for {name}')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "# Feature Importance (for Random Forest only)\n",
        "# Notes: Analyze which features are most important in the model.\n",
        "feature_importance = rf_pipeline.named_steps['classifier'].feature_importances_\n",
        "feature_names = rf_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "ax.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "ax.set_yticks(pos)\n",
        "ax.set_yticklabels(feature_names[sorted_idx])\n",
        "ax.set_xlabel('Feature Importance')\n",
        "ax.set_title('Feature Importance (MDI)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}